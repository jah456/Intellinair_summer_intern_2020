{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from skimage import data\n",
    "from skimage import color\n",
    "from skimage import morphology\n",
    "from skimage import segmentation\n",
    "from skimage import filters,io\n",
    "from skimage.future import graph\n",
    "from skimage.measure import regionprops, regionprops_table\n",
    "from skimage import draw\n",
    "from skimage import feature\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from spektral.layers import GraphConv, TopKPool,DiffPool, SAGPool,MinCutPool\n",
    "from spektral.layers import GraphAttention, GlobalAttentionPool\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "from spektral.utils import normalized_laplacian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOME_DIR = \"/home/ubuntu/\"\n",
    "\n",
    "\n",
    "INPUT = HOME_DIR + \"Dataset/fields_with_masks/\"\n",
    "\n",
    "Defined_CLASSES = [1]\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "l2_reg = 5e-4         # Regularization rate for l2\n",
    "learning_rate = 1e-3  # Learning rate for SGD\n",
    "batch_size = 2       # Batch size\n",
    "epochs = 10       # Number of training epochs\n",
    "es_patience = 200     # Patience fot early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(True)\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session  \n",
    "\n",
    "config =  tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "\"\"\" This function used to visualize the input, predicted mask and ground truth\"\"\"\n",
    "def visualize(save_status, save_path,flight_code,**images ):\n",
    "    \n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(40,40))\n",
    "    for i, (name , image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        # plt.title(str(i+1))\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        \n",
    "        plt.imshow(image,cmap='gray')\n",
    "    \n",
    "    if save_status : \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        plt.savefig(save_path+flight_code+'.jpg')    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def denormalize(x):\n",
    "    x_max = np.max(x)\n",
    "    x_min = np.min(x)\n",
    "    x = (x-x_min) / (x_max - x_min)\n",
    "    x = x.clip(0,1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "def compute_feats(image, kernels):\n",
    "    feats = []\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        feats.append(filtered.mean())\n",
    "        feats.append(filtered.std())\n",
    "    return feats\n",
    "def compute_rgb_hist_feats(image):\n",
    "    chans = cv2.split(image)\n",
    "\n",
    "    features = []\n",
    "    for chan in chans:\n",
    "        hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n",
    "        cv2.normalize(hist,hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        j = []\n",
    "        for i in hist:\n",
    "            j.append(np.mean(i))\n",
    "        j = np.asarray(j)    \n",
    "        features.append(np.mean(j))\n",
    "        \n",
    "    return features\n",
    "def make_ganbor_bank_kernels():\n",
    "    kernels = []\n",
    "    for theta in range(4):\n",
    "        theta = theta / 4. * np.pi\n",
    "        for sigma in (1, 3):\n",
    "            for frequency in (0.05, 0.25):\n",
    "                kernel = np.real(gabor_kernel(frequency, theta=theta,\n",
    "                                              sigma_x=sigma, sigma_y=sigma))\n",
    "                kernel = np.stack((kernel,kernel,kernel),axis = 2)\n",
    "                #print(kernel.shape)\n",
    "                kernels.append(kernel)\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = make_ganbor_bank_kernels()\n",
    "def find_bounding_contour_and_cal_features(segments,image, prev_node_no,nutrient_mask):\n",
    "    # loop over the unique segment values\n",
    "    image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    all_features = []\n",
    "    all_labels= []\n",
    "    ##for node 0\n",
    "    list_zeros = (np.zeros(9)).tolist()\n",
    "    all_features.append(list_zeros)\n",
    "    all_labels.append(0)\n",
    "    for (i, segVal) in enumerate(np.unique(segments)):\n",
    "    # construct a mask for the segment\n",
    "        if i > 0:\n",
    "            #print(\"[x] inspecting segment %d\" % (i))\n",
    "            mask = np.zeros(image_copy.shape[:2], dtype = \"uint8\")\n",
    "            mask[segments == segVal] = 255\n",
    "        #   # show the masked region\n",
    "        #   cv2.imshow(\"Mask\", mask)\n",
    "        #   cv2.imshow(\"Applied\", cv2.bitwise_and(image, image, mask = mask))\n",
    "            result = cv2.bitwise_and(image_copy, image_copy, mask = mask)\n",
    "            #print(cv2.findContours(result,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE))\n",
    "            cnt = cv2.findContours(result,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            x,y,w,h = cv2.boundingRect(cnt[0])\n",
    "            #print('x:{},y:{},w:{},h:{}'.format(x,y,w,h))\n",
    "            roi = image[y:y+w, x:x+h]\n",
    "            roi_mask = nutrient_mask[y:y+w, x:x+h]\n",
    "            roi_feats =[]#compute_feats(image=roi, kernels=kernels)\n",
    "            roi_feats.extend((np.mean(roi, axis=(0, 1))).tolist())\n",
    "            roi_feats.extend((np.std(roi, axis=(0, 1))).tolist())\n",
    "            roi_feats.extend((compute_rgb_hist_feats(roi)))\n",
    "            all_features.append(roi_feats)\n",
    "            if cv2.countNonZero(roi_mask) > 0: #0.5*w*h:\n",
    "                label = cv2.countNonZero(roi_mask) / (w*h)\n",
    "                all_labels.append(label)\n",
    "            else:\n",
    "                all_labels.append(0)\n",
    "            #if image is not None:\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "                #cv2.drawContours(image, cnt[0], -1, (0,255,0), 1)\n",
    "                #cv2.rectangle(image,(x,y),(x+w,y+h), (255,0,0), 2)\n",
    "                #cv2.imshow(\"\", image)\n",
    "               # cv2.imshow(\"roi\",roi)\n",
    "                #cv2.waitKey(1000)\n",
    "        #   cv2.waitKey(0)\n",
    "    while prev_node_no < 401:\n",
    "        list_zero = (np.zeros(9)).tolist()\n",
    "        all_features.append(list_zeros)\n",
    "        all_labels.append(0)\n",
    "        prev_node_no += 1\n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_edges(image, g, threshold):\n",
    "    edges = []\n",
    "    for n1, n1_neigh in g._adj.items():\n",
    "        for n2 in n1_neigh:\n",
    "            #print(str(n1) + ':', str(n2))\n",
    "            edges.append((n1,n2))\n",
    "\n",
    "    image = image.copy()\n",
    "    for edge in edges:\n",
    "        n1, n2 = edge\n",
    " \n",
    "        r1, c1 = map(int, g._node[n1]['centroid'])\n",
    "        r2, c2 = map(int, g._node[n2]['centroid'])\n",
    " \n",
    "        line  = draw.line(r1, c1, r2, c2)\n",
    "        circle = draw.circle(r1,c1,4)\n",
    " \n",
    "        if g._adj[n1][n2]['weight'] < threshold :\n",
    "            image[line] = 0,1,1\n",
    "        image[circle] = 1,1,0\n",
    " \n",
    "    return image\n",
    "\n",
    "def rag_mean_distance_color(image, labels, connectivity=1, mode='distance', sigma=255.0, threshold=5):\n",
    "    def np_hist_to_cv(np_histogram_output):\n",
    "        counts, bin_edges = np_histogram_output\n",
    "        return counts.ravel().astype('float32')\n",
    "    \n",
    "    image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    g = graph.RAG(labels, connectivity=connectivity)\n",
    "\n",
    "    for n in g:\n",
    "        g.nodes[n].update({'labels': [n],\n",
    "                               'pixel count': 0,\n",
    "                               'total color': [],\n",
    "                               'hist': None})#np.array([0, 0, 0],\n",
    "                                                      #dtype=np.double)})\n",
    "\n",
    "    for index in np.ndindex(labels.shape):\n",
    "        current = labels[index]\n",
    "        g.nodes[current]['pixel count'] += 1\n",
    "        g.nodes[current]['total color'].append(image[index])\n",
    "    for n in g:\n",
    "        roi = np.asarray(g.nodes[n]['total color'], dtype=np.float32)\n",
    "        g.nodes[n]['hist'] = np.histogramdd(roi)\n",
    "        #print(np_hist_to_cv(g.nodes[n]['hist']))\n",
    "#         g.nodes[n]['mean color'] = (g.nodes[n]['total color'] /\n",
    "#                                        g.nodes[n]['pixel count'])\n",
    "    #all_sim = []\n",
    "    for x in g:\n",
    "        for y in g:\n",
    "            if x != y:\n",
    "                diff = cv2.compareHist(np_hist_to_cv(g.nodes[x]['hist']), np_hist_to_cv(g.nodes[y]['hist']), cv2.HISTCMP_BHATTACHARYYA)\n",
    "                #diff = np.linalg.norm(diff)\n",
    "                if mode == 'similarity':\n",
    "                    diff = math.e ** (-(diff ** 2) / sigma)\n",
    "                elif mode =='distance':\n",
    "                    diff = diff\n",
    "                else:\n",
    "                    raise ValueError(\"The mode '%s' is not recognised\" % mode)\n",
    "                sim = 1-diff\n",
    "                #all_sim.append(sim)\n",
    "                if sim >= threshold:\n",
    "                    if g.has_edge(x,y):\n",
    "                        g.edges[x, y]['weight'] = sim\n",
    "                    else:\n",
    "                        g.add_edge(x, y, weight=sim )\n",
    "                else:\n",
    "                    if g.has_edge(x,y):\n",
    "                        g.remove_edge(x, y)\n",
    "    return g#,all_sim\n",
    "\n",
    "def super_pixels(**kwargs):\n",
    "    \n",
    "    def show_img(img):\n",
    "        width = 10.0\n",
    "        height = img.shape[0]*width/img.shape[1]\n",
    "        f = plt.figure(figsize=(width, height))\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    image = kwargs.get('image', None)\n",
    "    #show_img(image)\n",
    "    mask = kwargs.get('mask',None)\n",
    "    \n",
    "    nutrient_mask = kwargs.get('nutrient_mask', None)\n",
    "    labels = segmentation.slic(image, mask = mask, compactness=30, n_segments=400)\n",
    "    #show_img(labels)\n",
    "    regions = regionprops(labels)\n",
    "    #regions_table = regionprops_table(labels)\n",
    "    #print(regions_table)\n",
    "    label_rgb = color.label2rgb(labels, image, kind='avg',bg_label=0)\n",
    "    \n",
    "    #show_img(label_rgb)\n",
    "    \n",
    "    label_rgb = segmentation.mark_boundaries(label_rgb, labels, (0, 0, 0))\n",
    "    \n",
    "    #rag = graph.rag_mean_color(image, labels,connectivity = 1 , mode = 'distance')#'similarity', sigma =255)\n",
    "    rag= rag_mean_distance_color(image, labels,connectivity = 1 , mode = 'distance', threshold=0 )\n",
    "    #plt.figure()\n",
    "    #plt.hist(all_diff, 100)\n",
    "    #plt.show()\n",
    "    prev_node_no = len(rag)\n",
    "    rag.remove_node(0)\n",
    "    rag.add_node(0)\n",
    "    while len(rag) < 401:\n",
    "        rag.add_node(len(rag),size=0)   \n",
    "#     for region in regions:\n",
    "#         #print(region['centroid'])\n",
    "#         rag._node[region['label']]['centroid'] = region['centroid']\n",
    "    A =nx.adjacency_matrix(rag)\n",
    "    A = A.toarray()\n",
    "    #A = normalized_laplacian(A)\n",
    "    X,y=find_bounding_contour_and_cal_features(labels,image,prev_node_no,nutrient_mask)\n",
    "    X= np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    #edges_drawn_all = display_edges(label_rgb, rag, np.inf )\n",
    "    #show_img(edges_drawn_all)\n",
    "    \n",
    "    return A,labels,X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   \n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key,reverse=True)     \n",
    "class Dataset:\n",
    "    \n",
    "    CLASSES = Defined_CLASSES\n",
    "    def __init__(\n",
    "            self,\n",
    "            field_dir,\n",
    "            status, #train,test,validation\n",
    "            classes = None,\n",
    "            augmentation = None,\n",
    "            preprocessing = None\n",
    "            ):\n",
    "        \"\"\" make a list of image directions  \"\"\"\n",
    "        files = ['bounday_mask.png','nutrient_mask.png']#,'ndvi.png','gndvi.png','ndwi.png']\n",
    "        self.field_dirs = [ids for ids in sorted(os.listdir(field_dir))]\n",
    "        self.field_fps = [os.path.join(field_dir ,field_id) for field_id in self.field_dirs if len(os.listdir(os.path.join(field_dir ,field_id))) >= 3 ]\n",
    "        \n",
    "        train, validate, test = np.split(self.field_fps, [int(.6*len(self.field_fps)), int(.8*len(self.field_fps))])\n",
    "        print(len(train))\n",
    "        print(len(validate))\n",
    "        print(len(test))\n",
    "        if status == 'train':\n",
    "            self.field_fps = train\n",
    "        elif status == 'validate':\n",
    "            self.field_fps = validate\n",
    "        elif status == 'test':\n",
    "            self.field_fps = test\n",
    "        \n",
    "        self.field_codes            = []\n",
    "        self.field_number            = []\n",
    "        self.field_images            = []\n",
    "        self.field_nutrient_mask     = []\n",
    "        self.field_boundary_mask     = []\n",
    "\n",
    "        for field_path in self.field_fps:\n",
    "            #if(field_path != '/home/ubuntu/Dataset/fields_with_masks/8849'):\n",
    "                #print('yes')\n",
    "            flight_dirs = sorted_alphanumeric(os.listdir(field_path))\n",
    "            codes  = []\n",
    "            number = []\n",
    "            images = []\n",
    "            nutrient_mask = []\n",
    "            boundary_mask = []\n",
    "\n",
    "            for j in flight_dirs:\n",
    "                flight_number, flight_code = j.split('_')\n",
    "                codes.append(flight_code)\n",
    "                number.append(flight_number)\n",
    "                images.append(os.path.join(field_path,j,flight_code+'.png'))\n",
    "                boundary_mask.append(os.path.join(field_path,j,files[0]))\n",
    "                nutrient_mask.append(os.path.join(field_path,j,files[1]))\n",
    "\n",
    "\n",
    "            self.field_codes.append(codes)\n",
    "            self.field_number.append(number)\n",
    "            self.field_images.append(images)\n",
    "            self.field_nutrient_mask.append(nutrient_mask)\n",
    "            self.field_boundary_mask.append(boundary_mask)\n",
    "\n",
    "        \n",
    "        \"\"\" set the class values and assign a augmentation and preprocessing method\"\"\"\n",
    "        # self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        self.class_values = classes\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "         \n",
    "        \"\"\" read images\"\"\"\n",
    "        image1 = cv2.imread(self.field_images[i][0])\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "        \n",
    "        boundary_mask = cv2.imread(self.field_boundary_mask[i][0])\n",
    "        boundary_mask = cv2.cvtColor(boundary_mask, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        \n",
    "        nutrient_mask = cv2.imread(self.field_nutrient_mask[i][0])\n",
    "        nutrient_mask = cv2.cvtColor(nutrient_mask, cv2.COLOR_BGR2GRAY) \n",
    "        \n",
    "        boundary_mask = denormalize(boundary_mask)\n",
    "        nutrient_mask = denormalize(nutrient_mask)\n",
    "\n",
    "        \n",
    "        \"\"\" extract certain classes from mask 1 \"\"\"\n",
    "        masks = [(boundary_mask == v) for v in self.class_values]\n",
    "        boundary_mask = np.stack(masks, axis = -1).astype('float')\n",
    "        \n",
    "        boundary_mask_for_slic = np.stack(masks, axis = 0).astype('float')\n",
    "        \n",
    "        masks = [(nutrient_mask == v) for v in self.class_values]\n",
    "        nutrient_mask = np.stack(masks, axis = -1).astype('float')\n",
    "        \n",
    "        data1 = {\"image\": image1, \"mask\": nutrient_mask }\n",
    "        \"\"\" apply augmentation \"\"\"\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(**data1)\n",
    "            image1 , nutrient_mask = sample['image'], sample['mask']\n",
    "         \n",
    "        data2 = {\"image\": image1, \"mask\": nutrient_mask }\n",
    "        \"\"\" apply preprocessing \"\"\"\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(**data2)\n",
    "            image1 , nutrient_mask = sample['image'], sample['mask']\n",
    "        \n",
    "        A, labels,X,y = super_pixels(image=image1, mask=boundary_mask_for_slic, nutrient_mask=nutrient_mask)\n",
    "             \n",
    "        #return image, nutrient_mask, self.image_dirs[i]\n",
    "        return A, labels, X,self.field_codes[i][0] ,y,nutrient_mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.field_images)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset = Dataset(INPUT,status='train', classes = Defined_CLASSES)\n",
    "flights = {}\n",
    "for i in range(10,20):\n",
    "    print(i)\n",
    "    A,pixel_labels, X, flight_code,Y,nutrient_mask = dataset[i]\n",
    "    #print(A)\n",
    "    #print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# s3client = boto3.client('s3')\n",
    "\n",
    "# response = s3client.get_object(Bucket='flight-ops', Key='saba.dadsetan/third_chunck_train_5.pickle')\n",
    "\n",
    "# body = response['Body'].read()\n",
    "# data = pickle.loads(body)\n",
    "\n",
    "\n",
    "# flight_codes = []\n",
    "# Adjacency = []\n",
    "# features = []\n",
    "# pixel_labels=[]\n",
    "# y_label = []\n",
    "# nutrient_mask = []\n",
    "# for code in data.keys():\n",
    "#     flight_codes.append(code)\n",
    "#     Adjacency.append(data[code]['A'])\n",
    "#     features.append(data[code]['X'])\n",
    "#     pixel_labels.append(data[code]['pixel_labels'])\n",
    "#     y_label.append(data[code]['Y'])\n",
    "#     nutrient_mask.append(data[code]['nutrient_mask'])\n",
    "    \n",
    "    \n",
    "bucket='flight-ops'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "77\n",
      "78\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.6/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.6/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.6/site-packages/numpy/core/_methods.py:207: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "Third Chunck Done... :D \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dataset = Dataset(INPUT,status='train', classes = Defined_CLASSES)\n",
    "# flights = {}\n",
    "# key='saba.dadsetan/pickles/first_chunk_train.pickle'\n",
    "# for i in range(0,75):\n",
    "#     print(i)\n",
    "#     A,pixel_labels, X, flight_code,Y,nutrient_mask = dataset[i]\n",
    "#     a = {'A':A, 'pixel_labels':pixel_labels, 'X':X, 'Y':Y, 'nutrient_mask':nutrient_mask}\n",
    "#     flights[flight_code]= a\n",
    "\n",
    "# pickle_byte_obj = pickle.dumps(flights, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# s3_resource = boto3.resource('s3')\n",
    "# s3_resource.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "# print('First Chunck Done... :D ')\n",
    "# del(flights)\n",
    "\n",
    "# flights = {}\n",
    "# key='saba.dadsetan/pickles/second_chunk_train.pickle'\n",
    "# for i in range(75,150):\n",
    "#     print(i)\n",
    "#     A,pixel_labels, X, flight_code,Y,nutrient_mask = dataset[i]\n",
    "#     a = {'A':A, 'pixel_labels':pixel_labels, 'X':X, 'Y':Y, 'nutrient_mask':nutrient_mask}\n",
    "#     flights[flight_code]= a\n",
    "# pickle_byte_obj = pickle.dumps(flights, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# s3_resource = boto3.resource('s3')\n",
    "# s3_resource.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "# print('Second Chunck Done... :D ')\n",
    "# del(flights)\n",
    "\n",
    "flights = {}\n",
    "key='saba.dadsetan/pickles/third_chunk_train.pickle'\n",
    "for i in range(150,len(dataset)):\n",
    "    print(i)\n",
    "    A,pixel_labels, X, flight_code,Y,nutrient_mask = dataset[i]\n",
    "    a = {'A':A, 'pixel_labels':pixel_labels, 'X':X, 'Y':Y, 'nutrient_mask':nutrient_mask}\n",
    "    flights[flight_code]= a\n",
    "pickle_byte_obj = pickle.dumps(flights, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "print('Third Chunck Done... :D ')\n",
    "del(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "77\n",
      "78\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "test Chunck Done... :D \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dataset = Dataset(INPUT,status='test', classes = Defined_CLASSES)\n",
    "flights = {}\n",
    "key='saba.dadsetan/pickles/test.pickle'\n",
    "for i in range(0,len(dataset)):\n",
    "    print(i)\n",
    "    A,pixel_labels, X, flight_code,Y,nutrient_mask = dataset[i]\n",
    "    a = {'A':A, 'pixel_labels':pixel_labels, 'X':X, 'Y':Y, 'nutrient_mask':nutrient_mask}\n",
    "    flights[flight_code]= a\n",
    "pickle_byte_obj = pickle.dumps(flights,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "print('test Chunck Done... :D ')\n",
    "del(flights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "77\n",
      "78\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "validate Chunck Done... :D \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "Dataset(INPUT,status='validate', classes = Defined_CLASSES)\n",
    "flights = {}\n",
    "key='saba.dadsetan/pickles/validate.pickle'\n",
    "for i in range(0,len(dataset)):\n",
    "    print(i)\n",
    "    A,pixel_labels, X, flight_code,Y,nutrient_mask = dataset[i]\n",
    "    a = {'A':A, 'pixel_labels':pixel_labels, 'X':X, 'Y':Y, 'nutrient_mask':nutrient_mask}\n",
    "    flights[flight_code]= a\n",
    "pickle_byte_obj = pickle.dumps(flights, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "print('validate Chunck Done... :D ')\n",
    "del(flights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_graph)",
   "language": "python",
   "name": "conda_graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
